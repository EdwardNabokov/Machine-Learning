{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy as dc\n",
    "from functools import reduce\n",
    "from typing import Union, List\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/edward/notebooks/data/Titanic_data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.concat([data, pd.get_dummies(data.loc[:, 'Pclass'])], axis=1)\n",
    "data.drop(['PassengerId', 'Pclass', 'Name', 'Parch', 'SibSp', 'Ticket', 'Fare', 'Cabin', 'Embarked'], \n",
    "          axis=1, \n",
    "          inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "data.interpolate(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = dc(data.values[:, 1:]), dc(data.values[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode 'male' and 'female' as 1 and 0\n",
    "_, encoded = np.unique(X[:, 0], return_inverse=True)\n",
    "X[:, 0] = encoded\n",
    "\n",
    "# add additianal axis. (714,) -> (714, 1)\n",
    "y = y[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   X: [1 22.0 0 0 1] y: [0]\n",
      "2   X: [0 38.0 1 0 0] y: [1]\n",
      "3   X: [0 26.0 0 0 1] y: [1]\n",
      "4   X: [0 35.0 1 0 0] y: [1]\n",
      "5   X: [1 35.0 0 0 1] y: [0]\n",
      "6   X: [1 54.0 1 0 0] y: [0]\n",
      "7   X: [1 2.0 0 0 1] y: [0]\n",
      "8   X: [0 27.0 0 0 1] y: [1]\n",
      "9   X: [0 14.0 0 1 0] y: [1]\n",
      "10  X: [0 4.0 0 0 1] y: [1]\n"
     ]
    }
   ],
   "source": [
    "# output first 10 of current sets\n",
    "for i, (x_, y_) in enumerate(zip(X, y)):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print('{:<3} X: {} y: {}'.format(i+1, x_, y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((714, 5), (714, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, rate=0.01,\n",
    "                       epochs=500,\n",
    "                       hidden_layers=[4, 6, 4],\n",
    "                       output_layer=None,\n",
    "                       regularization=0,\n",
    "                       poly_power=2):\n",
    "        \"\"\"\n",
    "        Base constructor for neural network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rate : float (default 0.01)\n",
    "            The learning rate.\n",
    "\n",
    "        epochs : int (default 500)\n",
    "            Quantity of iterations.\n",
    "\n",
    "        hidden_layers : list (default [4, 6, 4])\n",
    "            Neurons each of hidden layer.\n",
    "            4 neurons for first hidden layer, 6 - for second, etc...\n",
    "\n",
    "        output_layer : int (default None)\n",
    "            Quantity of output layer neurons.\n",
    "\n",
    "            Note: it must be initialized.\n",
    "\n",
    "        regularization : float (default 0)\n",
    "            Used to balance between underfitting and overfitting.\n",
    "\n",
    "            Note: the bigger value, the less model fits to the data (causes underfitting)\n",
    "                the smaller value, the more model fits to the data (causes overfitting)\n",
    "\n",
    "        poly_power : int (default 2)\n",
    "            Max power of polynomial features.\n",
    "\n",
    "        Variables\n",
    "        ---------\n",
    "        inputLS : int\n",
    "            Quantity of input layer neurons.\n",
    "\n",
    "            Note: input layer size is being initialized during fit function\n",
    "                in regard to Xtrain shape\n",
    "\n",
    "        weights : list\n",
    "            List of weights.\n",
    "\n",
    "            Note: Weights are kept in the list, where each element of the list\n",
    "                is weights between certain layers\n",
    "\n",
    "        J : list\n",
    "            History of loss function\n",
    "\n",
    "        J_valid : list\n",
    "            History of validation loss function\n",
    "\n",
    "        _z : list\n",
    "            Layers multiplied by weights are kept here.\n",
    "\n",
    "        _a : list\n",
    "            Applied sigmoid function on layers, that are in _z\n",
    "\n",
    "        _dJdW : list\n",
    "            Derivatives of cost function\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.rate = rate\n",
    "        self.epochs = epochs\n",
    "        self.inputLS = None\n",
    "        self.hiddenLS = hidden_layers\n",
    "        self.outputLS = output_layer\n",
    "        self.weights = []\n",
    "        self.regularization = regularization\n",
    "        self.poly = poly_power\n",
    "\n",
    "        self.J = []\n",
    "        self.J_valid = []\n",
    "        self._z = []\n",
    "        self._a = []\n",
    "        self._dJdW = []\n",
    "\n",
    "    def tanh(self, z: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n",
    "        return np.tanh(z)\n",
    "\n",
    "    def relu(self, z: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n",
    "        return np.maximum(np.zeros(z.shape[0]), z)\n",
    "\n",
    "    def sigmoid(self, z: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n",
    "        return 1 / (1 + np.e**(-z))\n",
    "\n",
    "    def sigmoid_derivative(self, z):\n",
    "        # Derivative of vanilla sigmoid function\n",
    "        return np.e**(-z) / (1 + np.e**(-z))**2\n",
    "\n",
    "    def normalization(self, X: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n",
    "        # normalize data\n",
    "        return (X - X.mean(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "\n",
    "    def cost(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute loss using the cost function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray, shape (n_samples, n_features)\n",
    "            Training vector, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "\n",
    "        y : numpy.ndarray, shape (n_samples, 1) or (n_samples, n_labels)\n",
    "            Target relative to X for classification or regression;\n",
    "            None for unsupervised learning.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            Shape of array (n_samples, 1) or (n_samples, n_labels)\n",
    "\n",
    "            Note: In case of multiclass,\n",
    "                this function returns in onehot encoded format.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> X = np.array([[1, 2, 3],\n",
    "                          [4, 5, 6]])\n",
    "        >>> y = np.array([[1, 0, 0],\n",
    "                          [0, 1, 0]])\n",
    "        >>> cost(X, y)\n",
    "        [[0.87, 0.23, 0.54],\n",
    "         [0.42, 0.98, 0.13]]\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # feedforward pass\n",
    "        h = self.feedforward(X)\n",
    "        \n",
    "        # add regularization\n",
    "        # square each of weights (theta)\n",
    "        sqr_weights = list(map(lambda x: np.sum(x ** 2), self.weights))\n",
    "        # sum up weights\n",
    "        sum_weights = reduce(lambda x, y: x+y, sqr_weights)\n",
    "        \n",
    "        return 0.5 * sum((y - h)**2) / X.shape[0] + \\\n",
    "            self.regularization / (2*X.shape[0]) * sum_weights\n",
    "        \n",
    "    def cost_derivative(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute loss using derivative of cost function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray, shape (n_samples, n_features)\n",
    "            Training vector, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "\n",
    "        y : numpy.ndarray, shape (n_samples, 1) or (n_samples, n_labels)\n",
    "            Target relative to X for classification or regression;\n",
    "            None for unsupervised learning.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            Shape of array (n_samples, 1) or (n_samples, n_labels)\n",
    "\n",
    "            Note: In case of multiclasses,\n",
    "                this function returns in onehot encoded format.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # clear all saved layers\n",
    "        del self._dJdW[:]\n",
    "        del self._a[:]\n",
    "        del self._z[:]\n",
    "\n",
    "        # calculate derivative of cost function\n",
    "        h = self.feedforward(X)\n",
    "\n",
    "        # last (output) layer\n",
    "        delta = -(y - h) * self.sigmoid_derivative(self._z[-1])\n",
    "\n",
    "        # add regularization\n",
    "        derivated = np.dot(self._a[-2].T, delta) + self.regularization * self.weights[-1]\n",
    "        self._dJdW.append(derivated)\n",
    "\n",
    "        # go back through all hidden layers\n",
    "        for i in reversed(range(1, len(self._a) - 1)):\n",
    "            delta = np.dot(delta, self.weights[i+1].T) * self.sigmoid_derivative(self._z[i])\n",
    "            # add regularization\n",
    "            derivated = np.dot(self._a[i - 1].T, delta) + self.regularization * self.weights[i]\n",
    "            self._dJdW.append(derivated)\n",
    "\n",
    "        # first (input) layer\n",
    "        delta = np.dot(delta, self.weights[1].T)\n",
    "\n",
    "        # add regularization\n",
    "        derivated = np.dot(X.T, delta) + self.regularization * self.weights[0]\n",
    "        self._dJdW.append(derivated)\n",
    "\n",
    "        # reverse list of derivations\n",
    "        self._dJdW = self._dJdW[::-1]\n",
    "\n",
    "        return self._dJdW\n",
    "\n",
    "    def feedforward(self, X):\n",
    "        \"\"\"\n",
    "        Compute feedforward pass through the neural network to derive a hypothesis.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray, shape (n_samples, n_features)\n",
    "            Training vector, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray, shape (n_samples, 1) or (n_samples, n_features)\n",
    "\n",
    "            Note: In case of multiclasses,\n",
    "                this function returns in onehot encoded format.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        h = X\n",
    "        for current_weight in self.weights:\n",
    "            z = np.dot(h, current_weight)\n",
    "            a = self.sigmoid(z)\n",
    "\n",
    "            self._z.append(z)\n",
    "            self._a.append(a)\n",
    "\n",
    "            h = a\n",
    "\n",
    "        return h\n",
    "\n",
    "    def encode_onehot(self, y):\n",
    "        \"\"\"\n",
    "        Encode training set of labels into onehot format.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : numpy.ndarray, shape (n_samples, 1)\n",
    "            The training set of labels.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            Shape (n_samples, n_labels).\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> y = np.array([[0],\n",
    "                          [2],\n",
    "                          [4],\n",
    "                          [1],\n",
    "                          [2]])\n",
    "        [[1, 0, 0, 0, 0],\n",
    "         [0, 0, 1, 0, 0],\n",
    "         [0, 0, 0, 0, 1],\n",
    "         [0, 1, 0, 0, 0],\n",
    "         [0, 0, 1, 0, 0]]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        names, encoded = np.unique(y, return_inverse=True)\n",
    "        m = np.full((encoded.shape[0], names.shape[0]), 10e-5)\n",
    "        m[np.arange(encoded.shape[0]), encoded] = 1\n",
    "\n",
    "        return m\n",
    "\n",
    "    def _shuffle(self, X, y):\n",
    "        \"\"\"\n",
    "        Shuffle given sets.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray, shape (n_samples, n_features)\n",
    "            Training vector, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "\n",
    "        y : numpy.ndarray, shape (n_samples, 1) or (n_samples, n_features)\n",
    "            Target relative to X for classification or regression.\n",
    "            None for unsupervised learning.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple(numpy.ndarray, numpy.ndarray)\n",
    "            Shuffled given sets.\n",
    "            Correspondence of features and labels is preserved.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # build an array with the same dimension\n",
    "        indexes = np.arange(X.shape[0])\n",
    "\n",
    "        # shuffle indexes\n",
    "        np.random.shuffle(indexes)\n",
    "\n",
    "        # assign\n",
    "        X, y = X[indexes], y[indexes]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def split_data(self, X, y, ratio):\n",
    "        \"\"\"\n",
    "        Split given sets in regard to ratio.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray, shape (n_samples, n_features)\n",
    "            Training vector, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "\n",
    "        y : numpy.ndarray, shape (n_samples, 1) or (n_samples, n_labels)\n",
    "            Target relative to X for classification or regression.\n",
    "            None for unsupervised learning.\n",
    "\n",
    "        ratio : float\n",
    "            Value in range (0, 1).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple(numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray)\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> X.shape, y.shape\n",
    "        ((100, 4), (100, 1))\n",
    "        >>> split_data(X, y, ratio=0.2)\n",
    "        ((80, 4), (80, 1), (20, 4), (20, 1))\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        X, y = self._shuffle(X, y)\n",
    "\n",
    "        size1 = int(X.shape[0] * ratio)\n",
    "        size2 = int(X.shape[0] - size1)\n",
    "\n",
    "        return X[:size2], y[:size2], X[-size1:], y[-size1:]\n",
    "\n",
    "    def _add_poly_features(self, X):\n",
    "        \"\"\"\n",
    "        Add polynomial features.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray, shape (n_samples, n_features)\n",
    "            Training vector, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "\n",
    "            Note: polynomial power is initialized during defining model.\n",
    "                  There is no need to call this function.\n",
    "                  In case of initializing the polynomial power,\n",
    "                    it runs automatically.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            Shape (n_samples, n_labels).\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> X = np.array([[2, 3],\n",
    "                          [4, 5],\n",
    "                          [3, 2],\n",
    "                          [....]])\n",
    "        >>> model = NeuralNetwork(poly_power=3)\n",
    "        >>> model._add_poly_features(X)\n",
    "        [[2, 3, 4, 9, 8, 27],\n",
    "         [4, 5, 16, 25, 64, 125],\n",
    "         [3, 2, 9, 4, 27, 8]]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        base_num_features = X.shape[1]\n",
    "        for power in range(2, self.poly+1):\n",
    "            new_feature = X[:, :base_num_features] ** power\n",
    "            X = np.concatenate([X, new_feature], axis=1)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def _randomize_weights(self):\n",
    "        \"\"\"\n",
    "        Randomize initial weights of neural network\n",
    "        in regard to shape of Xtrain, ytrain and hidden layers.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # randomize input weights\n",
    "        self.W = np.random.randn(self.inputLS, self.hiddenLS[0])\n",
    "        self.weights.append(self.W)\n",
    "\n",
    "        # randomize weights between all hidden layers\n",
    "        for i in range(len(self.hiddenLS)-1):\n",
    "            self.W = np.random.randn(self.hiddenLS[i], self.hiddenLS[i+1])\n",
    "            self.weights.append(self.W)\n",
    "\n",
    "        # randomize output weights\n",
    "        self.W = np.random.randn(self.hiddenLS[-1], self.outputLS)\n",
    "        self.weights.append(self.W)\n",
    "\n",
    "    def fit(self, X, y, normalize=False, validation_split=None):\n",
    "        \"\"\"\n",
    "        Preprocess data, generate weights and train model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray, shape (n_samples, n_features)\n",
    "            The training set of features.\n",
    "\n",
    "        y : numpy.ndarray, shape (n_samples, 1) or (n_samples, n_labels)\n",
    "            The training set of labels.\n",
    "\n",
    "        normilize : bool (default False)\n",
    "            Data normalization.\n",
    "\n",
    "        validation_split : float (default None)\n",
    "            Split given X and y train set into two sets\n",
    "                (train and validation) in regard to ratio.\n",
    "\n",
    "            Note: validation split ratio must be specified in range (0, 1).\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # variables\n",
    "        self.normalize = normalize\n",
    "        self.validation_split = validation_split\n",
    "\n",
    "        # normalize training set to boost convergence\n",
    "        if self.normalize:\n",
    "            X = self.normalization(X)\n",
    "\n",
    "        # onehot encoding, if output layer consists of more than one neuron\n",
    "        if self.outputLS > 1:\n",
    "            print('Onehot was used.')\n",
    "            y = self.encode_onehot(y)\n",
    "\n",
    "        # add polynomial features\n",
    "        if self.poly > 1:\n",
    "            X = self._add_poly_features(X)\n",
    "\n",
    "        # add bias\n",
    "        self.Xtrain = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\n",
    "        self.ytrain = y\n",
    "\n",
    "        # get input size\n",
    "        self.inputLS = self.Xtrain.shape[1]\n",
    "\n",
    "        # randomize weights\n",
    "        self._randomize_weights()\n",
    "\n",
    "        # to keep original data, with original sizes\n",
    "        self.Xprimary = self.Xtrain\n",
    "        self.yprimary = self.ytrain\n",
    "\n",
    "        # split into two groups: train and validation sets\n",
    "        if self.validation_split:\n",
    "            splitted_data = self.split_data(self.Xprimary, \\\n",
    "                                            self.yprimary, \\\n",
    "                                            self.validation_split)\n",
    "            self.Xtrain, self.ytrain, self.Xvalid, self.yvalid = splitted_data\n",
    "        \n",
    "        # training\n",
    "        for epoch in range(self.epochs):\n",
    "            dJdW = self.cost_derivative(self.Xtrain, self.ytrain)\n",
    "            cost = self.cost(self.Xtrain, self.ytrain)\n",
    "\n",
    "            # split every epoch to validate on different samples\n",
    "            if self.validation_split:\n",
    "                self.validation_loss = self.cost(self.Xvalid, self.yvalid)\n",
    "                splitted_data = self.split_data(self.Xprimary, \\\n",
    "                                                self.yprimary, \\\n",
    "                                                self.validation_split)\n",
    "                self.Xtrain, self.ytrain, self.Xvalid, self.yvalid = splitted_data\n",
    "\n",
    "            for i in range(len(self.weights)):\n",
    "                self.weights[i] = self.weights[i] - self.rate*dJdW[i]\n",
    "\n",
    "            # add cost to the history\n",
    "            self.J.append(sum(cost))\n",
    "            # in case of initializing validation split ratio, add valid cost to history\n",
    "            if self.validation_split:\n",
    "                self.J_valid.append(sum(self.validation_loss))\n",
    "\n",
    "            print(self._progress(epoch, sum(cost)))\n",
    "\n",
    "    def _progress(self, epoch, train_loss):\n",
    "        \"\"\"\n",
    "        Print out the current epoch and loss.\n",
    "        In case of adding validation split,\n",
    "            it prints out the current validation loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epoch : int\n",
    "            Current epoch (iteration)\n",
    "\n",
    "        train_loss : float or double\n",
    "            Current train loss\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        def _add_validation(m):\n",
    "            return '{}'.format(m) + \\\n",
    "                ' Validation loss: {:<13}'.format(round(sum(self.validation_loss), 7))\n",
    "\n",
    "        m = 'Epoch: {:<6} Loss: {:<13}'.format(epoch+1, round(train_loss, 7))\n",
    "\n",
    "        if self.validation_split:\n",
    "            return _add_validation(m)\n",
    "\n",
    "        return m\n",
    "\n",
    "    def predict(self, Xtest, normalize=False):\n",
    "        \"\"\"\n",
    "        Compute hypothesis using trained weights and given Xtest.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Xtest : numpy.ndarray\n",
    "            The test set of features.\n",
    "        \n",
    "        normilize : bool (default False)\n",
    "            Data normalization.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            Predicted value or multiple values.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # normalize training set to boost convergence\n",
    "        if normalize:\n",
    "            Xtest = self.normalization(Xtest)\n",
    "        \n",
    "        # add polynomial features\n",
    "        if self.poly > 1:\n",
    "            Xtest = self._add_poly_features(Xtest)\n",
    "\n",
    "        Xtest = np.concatenate([np.ones((Xtest.shape[0], 1)), Xtest], axis=1)\n",
    "\n",
    "        h = Xtest\n",
    "        for current_weight in self.weights:\n",
    "            z = np.dot(h, current_weight)\n",
    "            a = self.sigmoid(z)\n",
    "            h = a\n",
    "\n",
    "        return h\n",
    "\n",
    "    def convert_to_integers(self, y):\n",
    "        \"\"\"\n",
    "        Round to the nearest integer value.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : numpy.ndarray\n",
    "            Training set of the labels.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> y = np.array([[0.95, 0.5, 0.12]])\n",
    "        >>> convert_to_decimal(y)\n",
    "        [[1, 0, 0]]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        y[y == y.max(axis=1)[:, None]] = 1\n",
    "        y[y < y.max(axis=1)[:, None]] = 0\n",
    "        \n",
    "        # TODO\n",
    "        # y = np.rint(np.array(list(map(lambda x: round(x, 3), y.ravel()))))\n",
    "\n",
    "        return y\n",
    "\n",
    "    def show_history(self):\n",
    "        \"\"\"Plot the history of the cost function during all time of training.\"\"\"\n",
    "\n",
    "        plt.plot(np.arange(len(self.J)), self.J, c='r')\n",
    "        plt.plot(np.arange(len(self.J_valid)), self.J_valid, c='b')\n",
    "        plt.grid(1)\n",
    "        plt.title('Cost function')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend(['Train loss', 'Valid loss'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain, ytrain = dc(X), dc(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(rate=0.01, \n",
    "                      epochs=100, \n",
    "                      hidden_layers=[6, 3], \n",
    "                      output_layer=1,\n",
    "                      regularization=1.3,\n",
    "                      poly_power=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain, ytrain, Xtest, ytest = model.split_data(Xtrain, ytrain, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((572, 5), (572, 1), (142, 5), (142, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape, ytrain.shape, Xtest.shape, ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1      Loss: 0.2579535     Validation loss: 0.6719409    \n",
      "Epoch: 2      Loss: 0.2533575     Validation loss: 0.6474711    \n",
      "Epoch: 3      Loss: 0.2475383     Validation loss: 0.6349433    \n",
      "Epoch: 4      Loss: 0.2433855     Validation loss: 0.6217244    \n",
      "Epoch: 5      Loss: 0.2378367     Validation loss: 0.6171921    \n",
      "Epoch: 6      Loss: 0.234872      Validation loss: 0.6048025    \n",
      "Epoch: 7      Loss: 0.2334969     Validation loss: 0.5869699    \n",
      "Epoch: 8      Loss: 0.2296373     Validation loss: 0.5793829    \n",
      "Epoch: 9      Loss: 0.2275395     Validation loss: 0.5659557    \n",
      "Epoch: 10     Loss: 0.2268857     Validation loss: 0.5472251    \n",
      "Epoch: 11     Loss: 0.2216202     Validation loss: 0.5476547    \n",
      "Epoch: 12     Loss: 0.2183237     Validation loss: 0.5411018    \n",
      "Epoch: 13     Loss: 0.2164127     Validation loss: 0.529792     \n",
      "Epoch: 14     Loss: 0.2156711     Validation loss: 0.5141794    \n",
      "Epoch: 15     Loss: 0.2113931     Validation loss: 0.512976     \n",
      "Epoch: 16     Loss: 0.210425      Validation loss: 0.4994399    \n",
      "Epoch: 17     Loss: 0.2086928     Validation loss: 0.4892795    \n",
      "Epoch: 18     Loss: 0.2066753     Validation loss: 0.480934     \n",
      "Epoch: 19     Loss: 0.2038073     Validation loss: 0.4765984    \n",
      "Epoch: 20     Loss: 0.2013827     Validation loss: 0.4708793    \n",
      "Epoch: 21     Loss: 0.1992663     Validation loss: 0.4643928    \n",
      "Epoch: 22     Loss: 0.1969292     Validation loss: 0.4593993    \n",
      "Epoch: 23     Loss: 0.1967879     Validation loss: 0.4461209    \n",
      "Epoch: 24     Loss: 0.1936942     Validation loss: 0.4449468    \n",
      "Epoch: 25     Loss: 0.1933713     Validation loss: 0.4331787    \n",
      "Epoch: 26     Loss: 0.1903023     Validation loss: 0.4328111    \n",
      "Epoch: 27     Loss: 0.1873284     Validation loss: 0.4326669    \n",
      "Epoch: 28     Loss: 0.1885462     Validation loss: 0.4162958    \n",
      "Epoch: 29     Loss: 0.18745       Validation loss: 0.4091357    \n",
      "Epoch: 30     Loss: 0.1860818     Validation loss: 0.4036138    \n",
      "Epoch: 31     Loss: 0.1825377     Validation loss: 0.407421     \n",
      "Epoch: 32     Loss: 0.1814447     Validation loss: 0.4014873    \n",
      "Epoch: 33     Loss: 0.1801287     Validation loss: 0.3970745    \n",
      "Epoch: 34     Loss: 0.1807738     Validation loss: 0.3851516    \n",
      "Epoch: 35     Loss: 0.1791485     Validation loss: 0.3824971    \n",
      "Epoch: 36     Loss: 0.1751366     Validation loss: 0.3895655    \n",
      "Epoch: 37     Loss: 0.1761343     Validation loss: 0.3773356    \n",
      "Epoch: 38     Loss: 0.1751942     Validation loss: 0.3728708    \n",
      "Epoch: 39     Loss: 0.1741656     Validation loss: 0.3693757    \n",
      "Epoch: 40     Loss: 0.1709667     Validation loss: 0.3746551    \n",
      "Epoch: 41     Loss: 0.1701665     Validation loss: 0.3709501    \n",
      "Epoch: 42     Loss: 0.1706533     Validation loss: 0.3623657    \n",
      "Epoch: 43     Loss: 0.1690642     Validation loss: 0.3621971    \n",
      "Epoch: 44     Loss: 0.1675117     Validation loss: 0.3626045    \n",
      "Epoch: 45     Loss: 0.1670127     Validation loss: 0.358651     \n",
      "Epoch: 46     Loss: 0.1663803     Validation loss: 0.355551     \n",
      "Epoch: 47     Loss: 0.1666845     Validation loss: 0.3489786    \n",
      "Epoch: 48     Loss: 0.1655637     Validation loss: 0.3484616    \n",
      "Epoch: 49     Loss: 0.1644288     Validation loss: 0.3481974    \n",
      "Epoch: 50     Loss: 0.1637714     Validation loss: 0.3465189    \n",
      "Epoch: 51     Loss: 0.1615691     Validation loss: 0.3514203    \n",
      "Epoch: 52     Loss: 0.1646725     Validation loss: 0.3349503    \n",
      "Epoch: 53     Loss: 0.1609754     Validation loss: 0.3459612    \n",
      "Epoch: 54     Loss: 0.1618706     Validation loss: 0.339373     \n",
      "Epoch: 55     Loss: 0.1614824     Validation loss: 0.337314     \n",
      "Epoch: 56     Loss: 0.1581098     Validation loss: 0.3474062    \n",
      "Epoch: 57     Loss: 0.1598383     Validation loss: 0.3376208    \n",
      "Epoch: 58     Loss: 0.1590927     Validation loss: 0.3382239    \n",
      "Epoch: 59     Loss: 0.1592043     Validation loss: 0.3353552    \n",
      "Epoch: 60     Loss: 0.1587234     Validation loss: 0.3341199    \n",
      "Epoch: 61     Loss: 0.1559062     Validation loss: 0.343616     \n",
      "Epoch: 62     Loss: 0.1551754     Validation loss: 0.3448346    \n",
      "Epoch: 63     Loss: 0.1566893     Validation loss: 0.3372109    \n",
      "Epoch: 64     Loss: 0.1572213     Validation loss: 0.3337562    \n",
      "Epoch: 65     Loss: 0.1561493     Validation loss: 0.3360726    \n",
      "Epoch: 66     Loss: 0.1546015     Validation loss: 0.3407902    \n",
      "Epoch: 67     Loss: 0.1555798     Validation loss: 0.3354663    \n",
      "Epoch: 68     Loss: 0.1561418     Validation loss: 0.3318023    \n",
      "Epoch: 69     Loss: 0.1542639     Validation loss: 0.3377879    \n",
      "Epoch: 70     Loss: 0.1548089     Validation loss: 0.3348569    \n",
      "Epoch: 71     Loss: 0.1531829     Validation loss: 0.339838     \n",
      "Epoch: 72     Loss: 0.1550598     Validation loss: 0.3320726    \n",
      "Epoch: 73     Loss: 0.1524204     Validation loss: 0.3423187    \n",
      "Epoch: 74     Loss: 0.1520178     Validation loss: 0.3440979    \n",
      "Epoch: 75     Loss: 0.1530865     Validation loss: 0.3390856    \n",
      "Epoch: 76     Loss: 0.1509955     Validation loss: 0.3467056    \n",
      "Epoch: 77     Loss: 0.154477      Validation loss: 0.3330291    \n",
      "Epoch: 78     Loss: 0.1519695     Validation loss: 0.3430005    \n",
      "Epoch: 79     Loss: 0.1528123     Validation loss: 0.3396285    \n",
      "Epoch: 80     Loss: 0.1552078     Validation loss: 0.3302007    \n",
      "Epoch: 81     Loss: 0.1512896     Validation loss: 0.3448024    \n",
      "Epoch: 82     Loss: 0.151277      Validation loss: 0.3442471    \n",
      "Epoch: 83     Loss: 0.1543209     Validation loss: 0.3326681    \n",
      "Epoch: 84     Loss: 0.1518928     Validation loss: 0.3425397    \n",
      "Epoch: 85     Loss: 0.1539328     Validation loss: 0.333112     \n",
      "Epoch: 86     Loss: 0.1517516     Validation loss: 0.3404705    \n",
      "Epoch: 87     Loss: 0.1500343     Validation loss: 0.3467212    \n",
      "Epoch: 88     Loss: 0.1524382     Validation loss: 0.3368406    \n",
      "Epoch: 89     Loss: 0.1494054     Validation loss: 0.3489493    \n",
      "Epoch: 90     Loss: 0.1518859     Validation loss: 0.3383218    \n",
      "Epoch: 91     Loss: 0.1529001     Validation loss: 0.3346905    \n",
      "Epoch: 92     Loss: 0.1540051     Validation loss: 0.3303641    \n",
      "Epoch: 93     Loss: 0.1476599     Validation loss: 0.3550662    \n",
      "Epoch: 94     Loss: 0.1507464     Validation loss: 0.3421238    \n",
      "Epoch: 95     Loss: 0.1527582     Validation loss: 0.3337033    \n",
      "Epoch: 96     Loss: 0.1517817     Validation loss: 0.3369993    \n",
      "Epoch: 97     Loss: 0.1511276     Validation loss: 0.3391299    \n",
      "Epoch: 98     Loss: 0.1483136     Validation loss: 0.3498767    \n",
      "Epoch: 99     Loss: 0.1511225     Validation loss: 0.3387562    \n",
      "Epoch: 100    Loss: 0.1513941     Validation loss: 0.3372354    \n"
     ]
    }
   ],
   "source": [
    "model.fit(Xtrain, ytrain, normalize=True, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKtyuTQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2076b9fac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.show_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = model.predict(Xtest, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((142, 1), (142, 1))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest.shape, h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = np.rint(np.array(list(map(lambda x: round(x, 3), h.ravel()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ytest = ytest.ravel() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  81.6901408451\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', sum(ytest == h) / len(h) * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
